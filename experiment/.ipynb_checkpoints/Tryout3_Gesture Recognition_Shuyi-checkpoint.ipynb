{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsCWJ6klvu_d"
   },
   "source": [
    "# \"Smart\" Gesture Recognition\n",
    "\n",
    "\n",
    "**Business Proposal**\n",
    "\n",
    "Nowadays, technology has a pivotal role in creating virtual environments with virtual elements. We work collaboratively with real-world intelligent objects on a day-to-day basis. Innovations like the Internet of things (IoT) products, gesture recognition, and teleportation application have certainly made our life \"smarter\" and more accessible.   \n",
    "\n",
    "\n",
    "Embracing an \"implementing a smarter life\" mindset, Team College aims to develop a cool feature in the smart TV that can recognize five different gestures performed by the user to help users control the TV without using a remote. Based on this dataset, Team College wants to develop a well-rounded deep learning gesture recognition model which has applications in a spectrum of different industries beyond smart-TV. \n",
    "\n",
    "**Dataset Description**\n",
    "The data consists of a few hundred videos categorized into five classes: thumb up, thumb down, swipe left, swipe right, stop. The webcam mounted on the TV continuously monitors the gestures. Each motion corresponds to a specific command:    \n",
    " \n",
    "| Gesture | Corresponding Action |\n",
    "| --- | --- | \n",
    "| Thumbs Up | Increase the volume. |\n",
    "| Thumbs Down | Decrease the volume. |\n",
    "| Left Swipe | 'Jump' backwards 10 seconds. |\n",
    "| Right Swipe | 'Jump' forward 10 seconds. |\n",
    "| Stop | Pause the movie. |\n",
    "\n",
    "The training data consists of 19,890 images (663*30): there are 663 videos categorized into one of the five classes. Each video (typically 2-3 seconds long) is divided into a sequence of 30 frames(images). The testing data consists of a total of 3,000 images.\t  \n",
    "\n",
    "Various people have recorded these videos performing one of the five gestures in front of a webcam - like what the smart TV will use. Specifically, videos have two types of dimensions - either 360x360 or 120x160 (depending on the webcam used to record the videos) \n",
    "\n",
    "### Team College Objectives:\n",
    "1. **Data Preprocessing_Generator Establishment**:  Team College aim to build a function called \"Generator\" so that this fuction should be able to take a batch of videos as input without any error. Steps like cropping, resizing and normalization should be performed successfully.\n",
    "\n",
    "2. **Experimentation _ Model Architecture**: Team College aim to develop the best model that is able to train with high accuracy and without any errors. To achieve the goal, Conv3D with Maxpolling 3D, RNN, CNN LSTM with GRU (for augmentation) models will be used to tackle the problem. \n",
    "\n",
    "3. **Best Performing Model**: Further Development and Tryouts such as Generative Adversarial Networks (GAN) , Different Batch Size, Learning Rate and tansfer learning method will be applied to design the best performing Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housekeeping Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, Activation, LSTM\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from matplotlib import image\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "from imageio import imread\n",
    "import datetime\n",
    "\n",
    "import pathlib\n",
    "from os.path import isfile, join\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed as 30 to ensure the steady outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setp for GPU check, if you have it, lucky you; ifelse, this might take some time to compute :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not have a GPU :( So I'm just going to load the dataset and set the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Loading train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Jiayu/Desktop/MMAI/MMAI894DeepLearning/project/MMAI894/experiment\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Project_data/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c1d97e3042c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#data_dir = pathlib.Path(data_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Project_data/train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Project_data/train.csv'"
     ]
    }
   ],
   "source": [
    "data_path = 'Project_data'\n",
    "#data_dir = pathlib.Path(data_path)\n",
    "\n",
    "df = pd.read_csv(\"Project_data/train.csv\", header=None, sep=';')\n",
    "df = df[0:3]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(folder):\n",
    "    loaded_images = []\n",
    "    for img in os.listdir(f'./{data_path}/{folder}'):\n",
    "        img_data = image.imread(f'./{data_path}/{folder}/{img}').astype(np.float32)\n",
    "        image_resized=resize(img_data,(100, 100))\n",
    "        loaded_images.append(image_resized)\n",
    "#     loaded_images = np.concatenate(loaded_images, axis=0 )\n",
    "    #loaded_images = np.stack(loaded_images, axais=0)\n",
    "    return loaded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[3] = df[0].apply(lambda x: load_images(f'train/{x}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy==1.19.5\r\n",
      "numpydoc @ file:///tmp/build/80754af9/numpydoc_1605117425582/work\r\n"
     ]
    }
   ],
   "source": [
    "! pip freeze | grep numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: './Project_data/train/WIN_20180926_16_51_21_Pro_Thumbs_Down_new'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7dd8f1e8b0ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-9d00d1a787ac>\u001b[0m in \u001b[0;36mload_images\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mloaded_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./{data_path}/{folder}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mimg_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./{data_path}/{folder}/{img}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mimage_resized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloaded_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_resized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1488\u001b[0m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mimg_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1491\u001b[0m         return (_pil_png_to_float_array(image)\n\u001b[1;32m   1492\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImagePlugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImageFile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2890\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2891\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2892\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: './Project_data/train/WIN_20180926_16_51_21_Pro_Thumbs_Down_new'"
     ]
    }
   ],
   "source": [
    "load_images('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Test dataset and setup batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Project_data'\n",
    "data_dir = pathlib.Path(data_path)\n",
    "\n",
    "Test = pd.read_csv(\"Project_data/val.csv\", header=None, sep=';')\n",
    "Test = Test[0:3]\n",
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test[3] = Test[0].apply(lambda x: load_images(f'val/{x}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "Original dataset only contains \"trian\" and \"val\". Team College aim to split the data for a \"train-val-test\" dataset with ratio of 68%-17%-13% by splitting \"train\" dataset into \"80%-20%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    train_df = df.copy()\n",
    "    Y = to_categorical(train_df[2], 5)\n",
    "#     X = train_df.drop([0, 1, 2, 3], axis=1)\n",
    "    X = np.stack(train_df[3], axis=0)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2)\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data EDA\n",
    "Sample train and val dataset display as follow:\n",
    "- train_doc contains 662 datapoints\n",
    "- val-doc contains 100 datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "Following steps are followed:\n",
    "1. Resized the image and cropped smaller image shape at center for 120*120\n",
    "2. Normalized Images\n",
    "3. Built 3D and 2D filter function \n",
    "4. Established function to get batch data for preprocessing Convo3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xrANziuvu_x"
   },
   "outputs": [],
   "source": [
    "def cropResize(image, y, z):\n",
    "    h, w = image.shape\n",
    "    \n",
    "    # if smaller image crop at center for 120x120\n",
    "    if w == 160:\n",
    "        image = image[:120, 20:140]\n",
    "\n",
    "    # resize every image\n",
    "    return resize(image, (y,z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhGrETufvu_y"
   },
   "outputs": [],
   "source": [
    "def normalizeImage(image):\n",
    "    # applying normalization\n",
    "    return image/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qd1bxIZvu_y"
   },
   "outputs": [],
   "source": [
    "def preprocessImage(image, y, z):\n",
    "    return normalizeImage(cropResize(image, y, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNgZq2e-vu_z"
   },
   "outputs": [],
   "source": [
    "def make3dFilter(x):\n",
    "    return tuple([x]*3)\n",
    "\n",
    "def make2dFilter(x):\n",
    "    return tuple([x]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yPBgXECvu_z"
   },
   "outputs": [],
   "source": [
    "def getBatchData(source_path, t, batch, batch_size, img_tensor):\n",
    "    [x,y,z] = [len(img_tensor[0]),img_tensor[1], img_tensor[2]]\n",
    "    img_idx = img_tensor[0]\n",
    "    batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "    batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "    for folder in range(batch_size): # iterate over the batch_size\n",
    "        imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "        for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "            image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "            #crop the images and resize them. Note that the images are of 2 different shape \n",
    "            #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "\n",
    "            # separate preprocessImage function is defined for cropping, resizing and normalizing images\n",
    "            batch_data[folder,idx,:,:,0] = preprocessImage(image[:, :, 0], y, z)\n",
    "            batch_data[folder,idx,:,:,1] = preprocessImage(image[:, :, 1], y, z)\n",
    "            batch_data[folder,idx,:,:,2] = preprocessImage(image[:, :, 2], y, z)\n",
    "\n",
    "        batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "    return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGY4lrhhvu_w"
   },
   "source": [
    "## Generator Establishment\n",
    "Since the dataset contains 2 types of dimensions, a \"generator\" is build to preprocess the images for `2D` and `3D` dimensions, as well as create `a batch of video frames`. We experiment with `img_idx`, `y`,`z` and normalization such that to achieve a high accuracy outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eA6r1IMmvu_0"
   },
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size, img_tensor):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(folder_list)/batch_size)\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            yield getBatchData(source_path, t, batch, batch_size, img_tensor)\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        # checking if any remaining batches are there or not\n",
    "        if len(folder_list)%batch_size != 0:\n",
    "            # updated the batch size and yield\n",
    "            batch_size = len(folder_list)%batch_size\n",
    "            yield getBatchData(source_path, t, batch, batch_size, img_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzNpErbFvu_1"
   },
   "source": [
    "Note: a video is represented above in the generator as (number of images, height, width, number of channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/Users/sh264/Downloads/Project_data/train'\n",
    "val_path = '/Users/sh264/Downloads/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(train_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_test_sequences = len(val_doc)\n",
    "print('# Test sequences =', num_val_sequences)\n",
    "num_epochs = 10\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2bDaqWXvu_4",
    "outputId": "22dc4b0e-3f48-401c-f469-4298839963c9"
   },
   "outputs": [],
   "source": [
    "def getImgTensor(n_frames):\n",
    "    img_idx = np.round(np.linspace(0, 29, n_frames)).astype(int)\n",
    "    return [img_idx, 100, 100, 3]\n",
    "\n",
    "# define image tensor size\n",
    "img_tensor = getImgTensor(20)\n",
    "print ('# img_tensor =', img_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEzkB8pzvu_5"
   },
   "source": [
    "### Generator Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKXoahGbvu_7",
    "outputId": "3cdcf0c4-8eea-4e1d-e822-7c473f5a1c90"
   },
   "outputs": [],
   "source": [
    "# check complete batch shape\n",
    "sample_generator = generator(train_path, train_doc, batch_size, img_tensor)\n",
    "sample_batch_data, sample_batch_labels = next(sample_generator)\n",
    "print(sample_batch_data.shape)\n",
    "\n",
    "# validation batch sample\n",
    "sample_val_generator = generator(val_path, val_doc, batch_size, img_tensor)\n",
    "sample_val_batch_data, sample_val_batch_labels = next(sample_val_generator)\n",
    "print(sample_val_batch_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xl6e7d0Ovu_8",
    "outputId": "f431fb7f-88f7-497a-a4a5-e017f5faab17"
   },
   "outputs": [],
   "source": [
    "# plot generated sample images\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(sample_val_batch_data[16,10,:,:,:])   \n",
    "ax[1].imshow(sample_val_batch_data[25,10,:,:,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3HvPgOPvu_-"
   },
   "outputs": [],
   "source": [
    "def plotModelHistory(h):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,4))\n",
    "    ax[0].plot(h.history['loss'])   \n",
    "    ax[0].plot(h.history['val_loss'])\n",
    "    ax[0].legend(['loss','val_loss'])\n",
    "    ax[0].title.set_text(\"Train loss vs Validation loss\")\n",
    "\n",
    "    ax[1].plot(h.history['categorical_accuracy'])   \n",
    "    ax[1].plot(h.history['val_categorical_accuracy'])\n",
    "    ax[1].legend(['categorical_accuracy','val_categorical_accuracy'])\n",
    "    ax[1].title.set_text(\"Train accuracy vs Validation accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Max. Training Accuracy\", max(h.history['categorical_accuracy']))\n",
    "    print(\"Max. Validaiton Accuracy\", max(h.history['val_categorical_accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcGAuU41vu__"
   },
   "source": [
    "## Model Building\n",
    "Team College aim to develop the best model that is able to train with high accuracy and without any errors. To achieve the goal, following Models have been used for tryouts:\n",
    "\n",
    "1. `Conv3D` with `MaxPooling3D` for a 3D convolution model \n",
    "2. `TimeDistributed` while building a Conv2D + RNN model\n",
    "3. `CNN LSTM with GRU` for augmentation\n",
    "\n",
    "Moreover, for each model, we set the last layer is the softmax so that the least number of parameters can fit in the memory of the webcam. Hence to perform a good accuracy on the least number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlVvi0S9vvAA"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout, LSTM\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A0Y13yhvvAA"
   },
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jD0uh84WvvAB",
    "outputId": "19a9cef8-30ee-4ce2-9869-20d2fd58bfe5"
   },
   "outputs": [],
   "source": [
    "#write your model here\n",
    "def defineModel(img_tensor):\n",
    "    inputShape = (len(img_tensor[0]), img_tensor[1], img_tensor[2], img_tensor[3])\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv3D(16, make3dFilter(5), activation='relu', input_shape=inputShape),\n",
    "        MaxPooling3D(make3dFilter(2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Conv3D(32, make3dFilter(3), activation='relu'),\n",
    "        MaxPooling3D(pool_size=(1,2,2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Conv3D(64, make3dFilter(3), activation='relu'),\n",
    "        MaxPooling3D(pool_size=(1,2,2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "model = defineModel(img_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SFrztx4vvAE"
   },
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, img_tensor)\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TshGKBpyvvAF"
   },
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNW5OuAsvvAF"
   },
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnIglXSuRxNs"
   },
   "source": [
    "### Model Callbacks Setup1 _ CheckPoint Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWzVNhUhRqpL",
    "outputId": "fa5a4f82-483e-490d-83ad-9a495943280c"
   },
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "\n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_fre='epoch')\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "\n",
    "callbacks_list = [checkpoint, LR]\n",
    "callbacks_list = [LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Callbacks Setup2 _ Tensorboard Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "\n",
    "#filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint2 = tf.keras.callbacks.TensorBoard(log_dir='logs', histogram_freq=0, write_graph=True,write_images=False, write_steps_per_second=False, update_freq='epoch',\n",
    "    profile_batch=0, embeddings_freq=0, embeddings_metadata=None)\n",
    "\n",
    "LR2 = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "\n",
    "callbacks_list = [checkpoint, LR2]\n",
    "callbacks_list = [LR2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Callbacks Setup3 _ EarlyStop Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "\n",
    "#filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint3 = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, min_delta=0, verbose=1,mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "LR3 = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "\n",
    "callbacks_list = [checkpoint, LR3]\n",
    "callbacks_list = [LR3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hem7-SrSZme"
   },
   "source": [
    "### Test Batch Size & Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ez2ESV-vHPKA"
   },
   "outputs": [],
   "source": [
    "def testBatchSize(batch_size, n_frames):\n",
    "    img_tensor = getImgTensor(n_frames)\n",
    "    print(\"Using frames as\", img_tensor[0])\n",
    "    global callbacks_list\n",
    "    num_epochs = 3\n",
    "    train_generator = generator(train_path, train_doc, batch_size, img_tensor)\n",
    "    val_generator = generator(val_path, val_doc, batch_size, img_tensor)\n",
    "    if (num_train_sequences%batch_size) == 0:\n",
    "        steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "    else:\n",
    "        steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "    if (num_val_sequences%batch_size) == 0:\n",
    "        validation_steps = int(num_val_sequences/batch_size)\n",
    "    else:\n",
    "        validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "    model = defineModel(img_tensor)\n",
    "    model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                callbacks=callbacks_list, validation_data=val_generator, \n",
    "                validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " testBatchSize(batch_size=64, n_frames=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testBatchSize(batch_size=64, n_frames=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testBatchSize(batch_size=32, n_frames=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testBatchSize(batch_size=32, n_frames=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vh1IZvwiajxA"
   },
   "source": [
    "With above observations, we can see that batch size is not impacting training time but number of frames. We will use batch size of 64 as it seems optimal. It is commented as it takes a lot of time to run that is restricting for the notebook to completely run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRxSBVQgbPHo"
   },
   "source": [
    "### Model 1 - Frames-16, Epoc-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9Z5A_HNZgyP",
    "outputId": "68c3c78f-354e-43a9-8534-c8d9c61cdc21"
   },
   "outputs": [],
   "source": [
    "n_frames = 16\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "img_tensor = getImgTensor(n_frames)\n",
    "train_generator = generator(train_path, train_doc, batch_size, img_tensor)\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_tensor)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "inputShape = (len(img_tensor[0]), img_tensor[1], img_tensor[2], img_tensor[3])\n",
    "\n",
    "model1 = Sequential([\n",
    "    Conv3D(16, make3dFilter(5), activation='relu', input_shape=inputShape),\n",
    "    MaxPooling3D(make3dFilter(2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv3D(32, make3dFilter(3), activation='relu'),\n",
    "    MaxPooling3D(pool_size=(1,2,2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv3D(64, make3dFilter(3), activation='relu'),\n",
    "    MaxPooling3D(pool_size=(1,2,2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Dense(5, activation='softmax')\n",
    "], name=\"conv_3d1\")\n",
    "model1.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print(model1.summary())\n",
    "\n",
    "model1_history = model1.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "            callbacks=callbacks_list, validation_data=val_generator, \n",
    "            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfeBXCls1r7R",
    "outputId": "c7ad7297-9bdc-4b0b-816d-5c86c63ee254"
   },
   "outputs": [],
   "source": [
    "plotModelHistory(model1_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsjCzIj40suJ"
   },
   "source": [
    "### Model 2: Frames-30, Epocs-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I82f2MK3bNyZ",
    "outputId": "d96d1cc3-3336-4be5-b3d1-ac8616919333"
   },
   "outputs": [],
   "source": [
    "n_frames = 30\n",
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "img_tensor = getImgTensor(n_frames)\n",
    "train_generator = generator(train_path, train_doc, batch_size, img_tensor)\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_tensor)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "inputShape = (len(img_tensor[0]), img_tensor[1], img_tensor[2], img_tensor[3])\n",
    "\n",
    "model2 = Sequential([\n",
    "    Conv3D(16, make3dFilter(5), activation='relu', input_shape=inputShape),\n",
    "    MaxPooling3D(make3dFilter(2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv3D(32, make3dFilter(3), activation='relu'),\n",
    "    MaxPooling3D(pool_size=(1,2,2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv3D(64, make3dFilter(3), activation='relu'),\n",
    "    MaxPooling3D(pool_size=(1,2,2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Dense(5, activation='softmax')\n",
    "], name=\"conv_3d2\")\n",
    "model2.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print(model2.summary())\n",
    "\n",
    "model2_history = model2.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "            callbacks=callbacks_list, validation_data=val_generator, \n",
    "            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGXMP96h1zDT",
    "outputId": "7b5e1257-e660-4bc9-c5ab-db47d1f62eae"
   },
   "outputs": [],
   "source": [
    "plotModelHistory(model2_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_-TyE8YDCnP"
   },
   "source": [
    "Reduce parameters, with padding in Conv3D layers and filter of (3,3,3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_Fqznw7DwLr"
   },
   "source": [
    "### Model 3: Frames-30, Epocs-20, Reduced parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlmHHFrL8HYu",
    "outputId": "ddc9ce30-3bb0-45c3-cd23-fcf0fb319f23"
   },
   "outputs": [],
   "source": [
    "n_frames = 30\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "img_tensor = getImgTensor(n_frames)\n",
    "train_generator = generator(train_path, train_doc, batch_size, img_tensor)\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_tensor)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "inputShape = (len(img_tensor[0]), img_tensor[1], img_tensor[2], img_tensor[3])\n",
    "\n",
    "model3 = Sequential([\n",
    "    Conv3D(16, make3dFilter(3), padding='same', activation='relu', input_shape=inputShape),\n",
    "    MaxPooling3D(make3dFilter(2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv3D(32, make3dFilter(3), padding='same', activation='relu'),\n",
    "    MaxPooling3D(pool_size=(2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv3D(64, make3dFilter(3), padding='same', activation='relu'),\n",
    "    MaxPooling3D(pool_size=(2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Dense(5, activation='softmax')\n",
    "], name=\"conv_3d3\")\n",
    "model3.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print(model3.summary())\n",
    "\n",
    "model3_history = model3.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "            callbacks=callbacks_list, validation_data=val_generator, \n",
    "            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotModelHistory(model3_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50alkFqTbYVq"
   },
   "source": [
    "### Model 4: Frames-20, Same pooling in Conv3D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kR92LCmTBQCD",
    "outputId": "ac73d2a2-58d4-4874-a4f7-89a6d76f0021"
   },
   "outputs": [],
   "source": [
    "n_frames = 20\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "img_tensor = getImgTensor(n_frames)\n",
    "train_generator = generator(train_path, train_doc, batch_size, img_tensor)\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_tensor)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "inputShape = (len(img_tensor[0]), img_tensor[1], img_tensor[2], img_tensor[3])\n",
    "\n",
    "model4 = Sequential([\n",
    "    Conv3D(16, make3dFilter(3), padding='same', activation='relu', input_shape=inputShape),\n",
    "    MaxPooling3D(make3dFilter(2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv3D(32, make3dFilter(3), padding='same', activation='relu'),\n",
    "    MaxPooling3D(pool_size=(2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv3D(64, make3dFilter(3), padding='same', activation='relu'),\n",
    "    MaxPooling3D(pool_size=(2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv3D(128, make3dFilter(3), padding='same', activation='relu'),\n",
    "    MaxPooling3D(pool_size=(2), padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Dense(5, activation='softmax')\n",
    "], name=\"conv_3d4\")\n",
    "model4.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print(model4.summary())\n",
    "\n",
    "model4_history = model4.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "            callbacks=callbacks_list, validation_data=val_generator, \n",
    "            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotModelHistory(model4_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jd5Nr0nUaR8X"
   },
   "source": [
    "### Model 5: Frames:20, Epocs-20, BS-64, reduced kernel to (2,2,2), switching BatchNormalization before MaxPooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuKTh7-yHs13",
    "outputId": "321c4671-f4b9-4a0a-ed65-5587ff3d92b8"
   },
   "outputs": [],
   "source": [
    "n_frames = 20\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "img_tensor = getImgTensor(n_frames)\n",
    "train_generator = generator(train_path, train_doc, batch_size, img_tensor)\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_tensor)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "inputShape = (len(img_tensor[0]), img_tensor[1], img_tensor[2], img_tensor[3])\n",
    "\n",
    "model5 = Sequential([\n",
    "    Conv3D(16, make3dFilter(2), padding='same', activation='relu', input_shape=inputShape),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling3D(make3dFilter(2)),\n",
    "\n",
    "    Conv3D(32, make3dFilter(2), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling3D(pool_size=(2)),\n",
    "\n",
    "    Conv3D(64, make3dFilter(2), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling3D(pool_size=(2)),\n",
    "\n",
    "    Conv3D(128, make3dFilter(2), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling3D(pool_size=(2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    # BatchNormalization(),\n",
    "    # Dropout(0.3),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    # BatchNormalization(),\n",
    "    # Dropout(0.2),\n",
    "\n",
    "    Dense(5, activation='softmax')\n",
    "], name=\"conv_3d5\")\n",
    "model5.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print(model5.summary())\n",
    "\n",
    "model5_history = model5.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "            callbacks=callbacks_list, validation_data=val_generator, \n",
    "            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotModelHistory(model5_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OFOJQZFcjQW"
   },
   "source": [
    "### Model 6: Switching Model architecture to Conv2D+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDuEGEx3TTyp",
    "outputId": "f0fc2581-3a52-4f03-ac21-f3b3d583ed9e"
   },
   "outputs": [],
   "source": [
    "n_frames = 20\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "img_tensor = getImgTensor(n_frames)\n",
    "train_generator = generator(train_path, train_doc, batch_size, img_tensor)\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_tensor)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "inputShape = (len(img_tensor[0]), img_tensor[1], img_tensor[2], img_tensor[3])\n",
    "\n",
    "model6 = Sequential([\n",
    "    TimeDistributed(Conv2D(16, make2dFilter(3), padding='same', activation='relu'), input_shape=inputShape),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D(make2dFilter(2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(32, make2dFilter(3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D(make2dFilter(2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(64, make2dFilter(3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D(make2dFilter(2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(128, make2dFilter(3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D(make2dFilter(2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(256, make2dFilter(3), padding='same', activation='relu')),\n",
    "    TimeDistributed(BatchNormalization()),\n",
    "    TimeDistributed(MaxPooling2D(make2dFilter(2))),\n",
    "\n",
    "    TimeDistributed(Flatten()),\n",
    "    LSTM(256),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(5, activation='softmax')\n",
    "], name=\"conv_2d_lstm\")\n",
    "model6.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print(model6.summary())\n",
    "\n",
    "model6_history = model6.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "            callbacks=callbacks_list, validation_data=val_generator, \n",
    "            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotModelHistory(model6_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skhLjh_vyXfR"
   },
   "source": [
    "### Model 7: Transfer Learning (MobileNet) with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6o_v7A10vBKd"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1o8quf9byzns",
    "outputId": "b4df59a3-3fe9-4cdd-ba7d-9dc69515603d"
   },
   "outputs": [],
   "source": [
    "mobilenet = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "\n",
    "n_frames = 20\n",
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "img_tensor = getImgTensor(n_frames)\n",
    "train_generator = generator(train_path, train_doc, batch_size, img_tensor)\n",
    "val_generator = generator(val_path, val_doc, batch_size, img_tensor)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "inputShape = (len(img_tensor[0]), img_tensor[1], img_tensor[2], img_tensor[3])\n",
    "\n",
    "model7 = Sequential([\n",
    "    TimeDistributed(mobilenet, input_shape=inputShape)\n",
    "], name=\"mobilenet_lstm\")\n",
    "\n",
    "for layer in model7.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model7.add(TimeDistributed(BatchNormalization()))\n",
    "model7.add(TimeDistributed(MaxPooling2D(make2dFilter(2))))\n",
    "model7.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model7.add(LSTM(256))\n",
    "model7.add(Dropout(0.2))\n",
    "\n",
    "model7.add(Dense(256,activation='relu'))\n",
    "model7.add(Dropout(0.2))\n",
    "\n",
    "model7.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model7.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print(model7.summary())\n",
    "\n",
    "model7_history = model7.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "            callbacks=callbacks_list, validation_data=val_generator, \n",
    "            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotModelHistory(model7_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
